# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/pyrnet/logger.ipynb.

# %% auto 0
__all__ = ['logger', 'dtype_gprmc', 'parse_gprmc', 'parse_adc', 'read_records', 'sync_adc_time', 'adc_binning',
           'interpolate_coords', 'read_logger']

# %% ../../nbs/pyrnet/logger.ipynb 3
from numpy.typing import NDArray,ArrayLike
import re
import gzip
import numpy as np
import pandas as pd
import xarray as xr
from scipy.stats import linregress
import logging
from toolz import assoc_in
import pkg_resources as pkg_res

from trosat import sunpos as sp

from . import utils
from . import reports

# logging setup
logging.basicConfig(
    filename='pyrnet.log',
    encoding='utf-8',
    level=logging.DEBUG,
    format='%(asctime)s %(name)s %(levelname)s:%(message)s'
)
logger = logging.getLogger(__name__)


# Local variables
_nat      = np.datetime64('NaT','ms')
_re_gprmc = re.compile('^(\d+,\d+ \d) \$GPRMC,(.*)(\*\w{2})$')
_re_adc   = re.compile('^(\d+)(\s\d+)+$')

# %% ../../nbs/pyrnet/logger.ipynb 12
def parse_gprmc(s, date_of_measure=np.datetime64('now')):
    '''
    Parse a string with a GPRMC GPS record

    Parameters
    ----------
    s: string
        A GPRMC record
    date_of_measure: datetime or datetime64
        A rough time, when the measurements happen to account for GPS rollover.
        Precise datetime is only necessary for the period of 2019-05-06 to 2019-08-17.
        Otherwise, providing a year is sufficient.
        The default is np.datetime64("now"), which is sufficient for all measurements conducted later than 2019-08-17.

    Returns
    -------
    gprmc : tuple
        A tuple with datetime64,status,lat,lon
    '''
    date_of_measure = utils.to_datetime64(date_of_measure)
    # split fields of GPRMC record
    f = s.split(',')
    status = f[1]
    if status=='A':
        try:
            # parse latitude
            lat = float(f[2][:2])+float(f[2][2:])/60
            if f[3]=='S':
                lat *= -1.0
            # parse longitude
            lon = float(f[4][:3])+float(f[4][3:])/60
            if f[5]=='W':
                lon *= -1.0
            if date_of_measure>np.datetime64("2019-04-06") and date_of_measure<np.datetime64("2019-08-17"): #account for gps week rollover
                YY = '19'+f[8][4:6]
            else:
                YY = "20"+f[8][4:6]
            mm = f[8][2:4]
            dd = f[8][0:2]
            HH = f[0][0:2]
            MM = f[0][2:4]
            SS = f[0][4:]
            dt = np.datetime64(YY+'-'+mm+'-'+dd+'T'+HH+':'+MM+':'+SS,'ms')
            if date_of_measure>np.datetime64("2019-04-06"): #account for gps week rollover -> date jump 1024 weeks back at 2019-04-06
                dt= dt+np.timedelta64(1024,'W')
            r = (dt,status,lat,lon)
        except:
            return (_nat,'V',np.nan,np.nan)
    else:
        r = (_nat,'V',np.nan,np.nan)
    return r

# %% ../../nbs/pyrnet/logger.ipynb 14
def parse_adc(s):
    '''
    Parse an ADC record

    Parameters
    ----------
    s: string
        The ADC record

    Returns
    -------
    t: tuple
       A tuple of digital counts of the ADC
    '''
    return tuple(map(int,s.split()))

# %% ../../nbs/pyrnet/logger.ipynb 16
dtype_gprmc = [
    ( 'time',   'datetime64[ms]' ),
    ( 'status', 'S1' ),
    ( 'lat',    'f8' ),
    ( 'lon',    'f8' ),
    ( 'iadc',   'u4' )
]

def read_records(fname: str,
                 date_of_measure: np.datetime64 = np.datetime64('now')) -> (NDArray, NDArray):
    '''
    Read the GPRMC and ADC records from the pyranometer logger files

    Parameters
    ----------
    fname: string
        The filename of the logger file
    date_of_measure: numpy.datetime64
        Date of measurement to account for gps rollover

    Returns
    -------
    rec_adc: ndarray
        The 10bit ADC readings
    rec_gprmc: recarray
        The GPRMC GPS records
    '''
    logger.info(f"Start reading records from file: {fname}")
    date_of_measure = utils.to_datetime64(date_of_measure)
    # Read file, use errors='ignore' to skip non UTF-8 characters
    # non UTF-8 characters may arise in broken GPS strings from time to time
    if fname[-3:]=='.gz':
        f = gzip.open(fname,'rt',errors='ignore') # open in text mode, ignore non UTF8 characters
    else:
        f = open(fname,'r',errors='ignore')
    lines =[l.rstrip() for l in f.readlines()]
    f.close()

    ##- skip almost empty files
    if len(lines)<20:
        logger.info("Skip file, as number of records is < 20.")
        return False,False

    # remove last line -> mostly damaged or empty
    lines=lines[:-1]
    # remove gps line at the end -> else processing issues
    if _re_gprmc.match(lines[-1]):
        lines=lines[:-1]

    rec_gprmc = []
    rec_adc = []
    iadc = 0
    for i,l in enumerate(lines):
        m = _re_gprmc.match(l)
        if m:
            r = parse_gprmc(m.group(2), date_of_measure)
            if not np.isnat(r[0]):
                # add number of adc values before GPS line
                rec_gprmc.append(r+(iadc,))
        elif _re_adc.match(l):
            r = parse_adc(l)
            if iadc==0:
                adc_len=len(r)
            # if record line is incomplete (due to power cut off)
            # the line is dropped
            if len(r)==adc_len:
                rec_adc.append(r)
                iadc += 1
        else:
            # unhandled record...
            pass
    rec_adc   = np.array(rec_adc,dtype=np.uint16)
    rec_gprmc = np.array(rec_gprmc,dtype=dtype_gprmc).view(np.recarray)
    logger.info("Done reading records from raw file.")
    return rec_adc, rec_gprmc

# %% ../../nbs/pyrnet/logger.ipynb 22
def sync_adc_time(rec_adc, rec_gprmc):
    '''
    Synchronize the ADC time to the GPS records

    Parameters
    ----------
    rec_adc: ndarray
        The digital counts of the ADC from the logger file
    rec_gprmc: recarray
        The GPRMC records from the logger file

    Returns
    -------
    time: ndarray(datetime64[ms])
        The time of the ADC records
    '''
    ### 1. Reconstruct logger clock
    # get millisecond part
    ta = rec_adc[:,0].astype('timedelta64[ms]')
    # get time difference between records
    dt = np.diff(ta)
    dt[dt<np.timedelta64(-850,'ms')] += 1000
    # get cummulative time offset rel. to first ADC record
    ta[0] = 0
    ta[1:] = np.cumsum(dt)
    ### 2. Get logger clock for GPRMC records
    # get time of previous ADC record
    t1 = ta[rec_gprmc.iadc]/np.timedelta64(1,'ms')
    # time of GPS records from GPRMC record
    t2 = (rec_gprmc.time-rec_gprmc.time[0])/np.timedelta64(1,'ms')
    a,b = linregress(t1,t2)[:2]
    t = rec_gprmc.time[0]+ta*a+b.astype('timedelta64[ms]')

    logger.info('Sync ADC time to GPS Fit Summary:')
    logger.info('|-- Drift  : {0:7.2f} [s/day]'.format( (1/a-1)*86400 ))
    logger.info('|-- Slope  : {0:13.8f}'.format(a))
    logger.info('|-- Offset : {0:7.2f} [s]'.format(b/1000))
    logger.info('|-- Jitter : {0:7.2f} [ms]'.format(np.std(t2-(a*t1+b))))
    return t

# %% ../../nbs/pyrnet/logger.ipynb 34
def adc_binning(rec_adc, time, bins=86400):
    """
    Binning and averaging of ADC samples

    Parameters
    ----------
    rec_adc: ndarray
        The ADC records parsed from the logger.
    time: ndarray of time objects
        Sample time of ADC records.
    bins: int
        Number of desired bins per day. The default is 86400, which result in
        mean values of 1 second steps per day. Maximum resolution is 86400000.

    Returns
    -------
    ndarray, ndarray(datetime64)
        Binned ADC records and corresponding time.
    """
    time = utils.to_datetime64(time)
    # starting day
    t0 = time[0].astype('datetime64[D]')
    # convert time to 'days from t0'
    dday = (time-t0)/np.timedelta64(1,'D')
    # calculate time bins of output dataset
    it = np.int64(dday*bins)
    # index for unique bins (inv_idx) and count of samples per bin (cnt)
    uval, inv_idx, cnt = np.unique(it,
                                   return_inverse=True,
                                   return_counts=True)
    logger.info(f"ADC records fill {len(uval)} bins of data.")
    # Calculate average of sample values per bin
    # The first two columns of rec_adc will be omitted as they store the
    # internal measures for timing and battery (first two columns)
    V = np.zeros((len(uval),rec_adc.shape[1]-2))
    for i in range(V.shape[1]):
        V[:,i] = np.bincount(inv_idx,weights=rec_adc[:,i+2])/cnt
    bintime = t0+ np.timedelta64(86400000,'ms')*uval.astype(np.float64)/bins
    logger.info(f"ADC records span a time period from {bintime[0]} to {bintime[-1]}.")
    return V, bintime

# %% ../../nbs/pyrnet/logger.ipynb 36
def interpolate_coords(rec_gprmc, time):
    """
    Interpolate lat and lon from gps records

    Parameters
    ----------
    rec_gprmc: recarray
        The GPRMC records from the logger file
    time: list or array of time objects

    Returns
    -------
    ndarray, ndarray
        lat and lon interpolated from GPS records to `time`
    """
    time = utils.to_datetime64(time)
    t0 = time[0].astype('datetime64[D]')
    # coordinate variables
    x1 = (time-t0)/np.timedelta64(1,'ms')
    x2 = (rec_gprmc.time-t0)/np.timedelta64(1,'ms')
    lat = np.interp(x1,x2,rec_gprmc.lat)
    lon = np.interp(x1,x2,rec_gprmc.lon)
    return lat, lon

# %% ../../nbs/pyrnet/logger.ipynb 39
def read_logger(
        fname : str,
        *,
        station: int,
        report: dict,
        bins : int = 86400,
        date_of_measure : np.datetime64 = np.datetime64("now"),
        config: dict|None = None,
        global_attrs: dict|None = None
) -> xr.Dataset:
    """
    Read logger binary file and parse it to xarray Dataset. Thereby, attributes and names are defined via cfmeta.json file and sun position values are calculated and added.

    Parameters
    ----------
    fname: str
        Path and filename of the raw logger file.
    station: int
        PyrNet station box number.
    report: dict
        Parsed maintenance report, see lib_reports.ipynb
    bins: int
        Number of desired bins per day. The default is 86400, which result in
        mean values of 1 second steps per day. Maximum resolution is 86400000.
    date_of_measure: float, datetime or datetime64
        A rough date of measure  to account for GPS week rollover. If measured in 2019, day resolution is recommended, before 2019 annual resolution, 2020 onwards not required. If float, interpreted as Julian day from 2000-01-01T12:00. the default is np.datetime64("now").
    config: dict
        Stores processing specific configuration.
            * cfjson -> path to cfmeta.json, the default is "../share/pyrnet_cfmeta_l1b.json"
            * stripminutes -> number of minutes to be stripped from the data at start and end,
                the default is 5.
    global_attrs: dict
        Additional global attributes for the Dataset. (Overrides cfmeta.json attributes)
    Returns
    -------
    xarray.Dataset
        Raw Logger data for one measurement periode.
    """
    fn_cfjson = pkg_res.resource_filename("pyrnet", "share/pyrnet_cfmeta_l1b.json")

    default_config = {
            "campaign": "",
            "project": "",
            "creator_name": "",
            "contributor_name": "",
            "contributor_role": "",
            "notes": "",
            "cfjson": fn_cfjson,
            "stripminutes": 5,
        }
    if config is None:
        config = default_config
    config = {**default_config, **config}


    if global_attrs is None:
        global_attrs = {}

    date_of_measure = utils.to_datetime64(date_of_measure)

    # 1. Parse binary
    rec_adc, rec_gprmc = read_records(fname=fname, date_of_measure=date_of_measure)

    if type(rec_adc)==bool or len(rec_gprmc.time)<3:
        logger.debug("Failed to load the data from the file, because of not enough stable GPS data, or file is empty.")
        return False

    # 2. Sync GPS to ADC time
    adctime = sync_adc_time(rec_adc, rec_gprmc)

    # 3. Apply appropriate binning to ADC values
    adc_counts, bintime = adc_binning(rec_adc, time=adctime, bins=bins)

    # 4. Interpolate GPS coordinates to bin time
    lat, lon = interpolate_coords(rec_gprmc, bintime)

    # 5. Get Logbook maintenance quality flags
    key = f"{station:03d}"
    if key not in report:
        logger.warning(f"No report for station {station} available.")
    qc_main = reports.get_qcflag(
        qc_clean=report[key]['clean'],
        qc_level=report[key]['align']
    )
    qc_extra = reports.get_qcflag(
        qc_clean=report[key]['clean2'],
        qc_level=report[key]['align2']
    )

    # 6. get metadata and encoding
    # parse the json file
    cfdict = utils.read_json(config["cfjson"])

    # get global attributes:
    gattrs = cfdict['attributes']

    # apply config
    gattrs = {k:v.format_map(config) for k,v in gattrs.items()}

    # add additional global attrs
    gattrs.update(global_attrs)

    duration = adctime[-1]-adctime[0]
    resolution = adctime[1] - adctime[0]
    now = pd.to_datetime(np.datetime64("now"))
    gattrs.update({
        'processing_level': 'l1a',
        # 'product_version': '',# TODO: add version
        'date_created': now.isoformat(),
        'geospatial_lat_min': np.nanmin(lat),
        'geospatial_lat_max': np.nanmax(lat),
        'geospatial_lat_units': 'degN',
        'geospatial_lon_min': np.nanmin(lon),
        'geospatial_lon_max': np.nanmax(lon),
        'geospatial_lon_units': 'degE',
        'time_coverage_start': pd.to_datetime(adctime[0]).isoformat(),
        'time_coverage_end': pd.to_datetime(adctime[-1]).isoformat(),
        'time_coverage_duration': pd.to_timedelta(duration).isoformat(),
        'time_coverage_resolution': pd.to_timedelta(resolution).isoformat(),
        'history': f'{now.isoformat()}: Generated level l1a  by pyrnet version XX; ', # TODO: add version info
    })

    # get variable attributes
    d = utils.get_var_attrs(cfdict)

    # split encoding attributes
    vattrs, vencode = utils.get_attrs_enc(d)

    # add qc notes
    vattrs = assoc_in(vattrs, ["dflx_sw_qc","note_general"], report[key]["note_general"])
    vattrs = assoc_in(vattrs, ["dflx_sw_2_qc","note_general"], report[key]["note_general"])
    vattrs = assoc_in(vattrs, ["dflx_sw_qc","note_clean"], report[key]["note_clean"])
    vattrs = assoc_in(vattrs, ["dflx_sw_2_qc","note_clean"], report[key]["note_clean2"])
    vattrs = assoc_in(vattrs, ["dflx_sw_qc","note_level"], report[key]["note_align"])
    vattrs = assoc_in(vattrs, ["dflx_sw_2_qc","note_level"], report[key]["note_align2"])

    # 7. Calc sun position
    szen, sazi = sp.sun_angles(bintime, lat, lon)
    esd = np.mean(sp.earth_sun_distance(bintime))

    # 8. Make xarray Dataset
    count2volt = lambda x: 3.3 * np.float64(x)/1023.
    ds = xr.Dataset(
        data_vars={
            "dflx_sw": (("time","station"), count2volt(adc_counts[:,2])[:,None]), # [V]
            "dflx_sw_2": (("time","station"), count2volt(adc_counts[:,4])[:,None]), # [V]
            "ta_atm": (("time","station"), 253.15 + 20.*2.*count2volt(adc_counts[:,0])[:,None]), # [K]
            "rh_atm": (("time","station"), 0.2*2.*count2volt(adc_counts[:,1])[:,None]), # [-]
            "battery_voltage": (("time","station"), 2.*count2volt(adc_counts[:,3])[:,None]), # [V]
            "lat": (("time","station"), lat[:,None]), # [degN]
            "lon": (("time","station"), lon[:,None]), # [degE]
            "dflx_sw_qc": ("station", [qc_main]),
            "dflx_sw_2_qc": ("station", [qc_extra]),
            "szen": (("time","station"), szen[:,None]),
            "sazi": (("time","station"), sazi[:,None]),
            "esd": esd,
        },
        coords={
            "time": ("time", bintime),
            "station": ("station", [station]),
        },
        attrs=gattrs
    )

    # add attributes to Dataset
    for k,v in vattrs.items():
        if k not in ds.keys():
            continue
        ds[k].attrs = v

    # add encoding to Dataset
    for k,v in vencode.items():
        if k not in ds.keys():
            continue
        ds[k].encoding = v

    ds["time"].encoding.update({
        "units": f"seconds since {np.datetime_as_string(ds.time.data[0], unit='D')}T00:00Z",
    })

    # 9. Drop first and last <stripminutes> minutes of data to avoid bad data due to maintenance
    stripminutes = np.timedelta64(int(config['stripminutes']), 'm')
    tslice = slice(ds.time.values[0] + stripminutes,
                   ds.time.values[-1] - stripminutes)
    ds = ds.sel(time=tslice)

    return ds

